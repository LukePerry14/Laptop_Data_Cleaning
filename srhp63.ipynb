{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Worksheet Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from fuzzywuzzy import process\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WorkSheet Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "UncleanData = \"amazon_laptop_2023.xlsx\"\n",
    "CleanData = \"amazon_laptop_2023_cleaned.xlsx\"\n",
    "# Read the data from the Excel file\n",
    "df = pd.read_excel(UncleanData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_brand_model(Features):\n",
    "    #establish columns\n",
    "    brand = str(Features['brand'])\n",
    "    model = str(Features['model'])\n",
    "    #check brand isn't already in model\n",
    "    if brand not in model:\n",
    "        return brand + ' ' + model\n",
    "    else:\n",
    "        return model \n",
    "    \n",
    "def process_storage(value):\n",
    "    #check if value contains unit\n",
    "    if type(value) == str:\n",
    "\n",
    "        #remove unit\n",
    "        vals = value.split(' ')\n",
    "        vals[0] = float(vals[0])\n",
    "\n",
    "        #check if value needs converting to gb and return converted value\n",
    "        if (vals[1] == 'tb'):\n",
    "            return int(vals[0]) * 1000\n",
    "        \n",
    "        #if no conversion needed return value\n",
    "        return str(round(vals[0]))\n",
    "    return str(value)\n",
    "\n",
    "def regression_with_price(df, target):\n",
    "    #select datapoints with values for price to train regression model\n",
    "    X = df.dropna(subset=[target])[['price']]\n",
    "    y = df.dropna(subset=[target])[target]\n",
    "\n",
    "    #train model\n",
    "    tree_model = DecisionTreeRegressor()\n",
    "    tree_model.fit(X, y)\n",
    "\n",
    "    #identify target records for regression\n",
    "    X_missing = df[df[target].isna()][['price']]\n",
    "\n",
    "    #predict values\n",
    "    predicted_values = tree_model.predict(X_missing)\n",
    "    return predicted_values\n",
    "\n",
    "def regression_with_price_test(df, target):\n",
    "    #select datapoints with values for 'price' to train the regression model\n",
    "    X = df.dropna(subset=[target])[['price']]\n",
    "    y = df.dropna(subset=[target])[target]\n",
    "\n",
    "    #split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)\n",
    "\n",
    "    #train model\n",
    "    tree_model = DecisionTreeRegressor()\n",
    "    tree_model.fit(X_train, y_train)\n",
    "\n",
    "    #identify target records for regression\n",
    "    X_missing = df[df[target].isna()][['price']]\n",
    "\n",
    "    #predict values\n",
    "    predicted_values = tree_model.predict(X_missing)\n",
    "\n",
    "    #evaluate the model on the testing set\n",
    "    y_pred_test = tree_model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    print(f'Mean Squared Error for {target}: {mse}')\n",
    "    print(f'R-squared on {target}: {r2}\\n\\n')\n",
    "\n",
    "    return predicted_values\n",
    "\n",
    "def rating_regression_test(df):\n",
    "    #function to test the use of a regression model for the prediction of the rating values by evaluating the MSE and R-sqaured value\n",
    "\n",
    "    #take copies of df to ensure original values unchanged\n",
    "    test = df.copy()\n",
    "    test = test.dropna(subset=['rating'])\n",
    "\n",
    "    #select columns for data\n",
    "    X = test[['screen_size', 'harddisk', 'ram', 'OS', 'graphics']]\n",
    "    y = test['rating']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    #preprocess categorical features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), ['OS', 'graphics'])\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "    X_test_encoded = preprocessor.transform(X_test)\n",
    "\n",
    "    #train model with data\n",
    "    decision_tree_model = DecisionTreeRegressor()\n",
    "    decision_tree_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "    #make predictions and evaluate performance\n",
    "    y_pred = decision_tree_model.predict(X_test_encoded)\n",
    "    y_pred_rescaled = (y_pred - np.min(y_pred)) / (np.max(y_pred) - np.min(y_pred)) * 5\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred_rescaled)\n",
    "    r2 = r2_score(y_test, y_pred_rescaled)\n",
    "\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    print(f'R-squared: {r2}')\n",
    "\n",
    "def regression_on_price_test(df):\n",
    "    #function to test the use of a regression model for the prediction of the price values by evaluating the MSE and R-sqaured value\n",
    "\n",
    "    #take copies of df to ensure original values unchanged\n",
    "    test = df.copy()\n",
    "    test = test.dropna(subset=['price'])\n",
    "\n",
    "    #select columns for data\n",
    "    X = test[['screen_size', 'harddisk', 'ram', 'OS', 'graphics']]\n",
    "    y = test['price']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    #preprocess categorical features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), ['OS', 'graphics'])\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "    X_test_encoded = preprocessor.transform(X_test)\n",
    "\n",
    "    #train model with data\n",
    "    decision_tree_model = DecisionTreeRegressor()\n",
    "    decision_tree_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "    #make predictions and evaluate performance\n",
    "    y_pred = decision_tree_model.predict(X_test_encoded)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    print(f'R-squared: {r2}')\n",
    "\n",
    "def regression_on_price(df):\n",
    "\n",
    "    #filter df to get training data for regression model and prediction data\n",
    "    train_data = df.dropna(subset=['price'])\n",
    "    predict_data = df[df['price'].isna()]\n",
    "    X_train = train_data[['screen_size', 'harddisk', 'ram', 'OS', 'graphics']]\n",
    "    y_train = train_data['price']\n",
    "    X_predict = predict_data[['screen_size', 'harddisk', 'ram', 'OS', 'graphics']]\n",
    "\n",
    "    #preprocess categorical features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), ['OS', 'graphics'])\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "    X_predict_encoded = preprocessor.transform(X_predict)\n",
    "\n",
    "    #train model with data\n",
    "    decision_tree_model = DecisionTreeRegressor()\n",
    "    decision_tree_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "    #make predictions for records without prices rounded to 2dp\n",
    "    predicted_prices = decision_tree_model.predict(X_predict_encoded).round(2)\n",
    "\n",
    "    #reinsert data into df\n",
    "    df.loc[df['price'].isna(), 'price'] = predicted_prices\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def cpu_shrink(cpu_column):\n",
    "    #initial set of allowed values\n",
    "    allowed_cpus = ['core m', 'core i3', 'core i5', 'core i7', 'core i9', 'ryzen 3', 'ryzen 5', 'ryzen 7', 'ryzen 9', 'celeron', 'ryzen r series']\n",
    "\n",
    "\n",
    "    for i in range(len(cpu_column)):\n",
    "        value = cpu_column.iloc[i]\n",
    "\n",
    "        #maintain unknown values\n",
    "        if (value == \"unknown\") or (value == \"nan\") or (value == \"others\"):\n",
    "            cpu_column.iat[i] = \"nan\"\n",
    "            continue\n",
    "        \n",
    "        #run fuzzy string matching on value\n",
    "        result = process.extractOne(str(value), allowed_cpus)\n",
    "        match_value, score = result\n",
    "\n",
    "        #if matching confidence less than 90% insert value into set of allowed cpus, otherwise use matched value.\n",
    "        if score < 90:\n",
    "            allowed_cpus.append(value)\n",
    "            cpu_column.iat[i] = value\n",
    "        else:\n",
    "            cpu_column.iat[i] = match_value\n",
    "\n",
    "    #return shrunk column\n",
    "    return cpu_column\n",
    "\n",
    "def os_shrink(os_column):\n",
    "    #initial set of allowed values\n",
    "    allowed_os = ['windows 7', 'windows 8', 'windows 10 home', 'windows 10 pro', 'windows 11 home', 'windows 11 pro', 'mac os', 'chrome os']\n",
    "\n",
    "\n",
    "    for i in range(len(os_column)):\n",
    "        value = os_column.iloc[i]\n",
    "\n",
    "        #maintain unknown values\n",
    "        if (value == \"unknown\") or (value == \"nan\") or (value == \"others\"):\n",
    "            os_column.iat[i] = \"nan\"\n",
    "            continue\n",
    "\n",
    "        #run fuzzy string matching on value\n",
    "        result = process.extractOne(str(value), allowed_os)\n",
    "        match_value, score = result\n",
    "\n",
    "        os_column.iat[i] = match_value\n",
    "\n",
    "    #return shrunk column\n",
    "    return os_column\n",
    "\n",
    "def special_shrink(special_column):\n",
    "    #initial set of allowed values\n",
    "    allowed_special = ['wifi', 'bluetooth', 'anti glare', 'fingerprint reader', 'backlit keyboard', 'hd audio', 'stylus', 'security slot', 'memory card slot','bezel', 'corning gorilla glass']\n",
    "\n",
    "\n",
    "    for i in range(len(special_column)):\n",
    "        inp = []\n",
    "\n",
    "        #leave unknown values as an empty set of features\n",
    "        if (str(special_column.iloc[i]) == \"nan\"):\n",
    "            special_column.iat[i] = []\n",
    "            continue\n",
    "\n",
    "        #convert string into array of features\n",
    "        values = special_column.iloc[i].split(',')\n",
    "\n",
    "        for value in values:\n",
    "            value = value.strip()\n",
    "\n",
    "            #handle obvious pitfalls i.e. n/a values, sentences which contain commas, and empty values\n",
    "            if (value == \"information not available\"):\n",
    "                special_column.iat[i] = []\n",
    "                break\n",
    "            elif (value.count(\" \") > 3) or (value == \"\"):\n",
    "                continue\n",
    "            \n",
    "            #run fuzzy string matching on value\n",
    "            result = process.extractOne(str(value), allowed_special)\n",
    "            match_value, score = result\n",
    "\n",
    "            #if matching confidence less than 90% insert value into set of allowed values(unless one of the major outliers that already matches a value in the allowed array), otherwise use matched value and add to the features array for that record.\n",
    "            if score < 90:\n",
    "                if(value == \"backlit kb\"):\n",
    "                    inp.append(\"backlit keyboard\")\n",
    "                elif(value == \"fingerprint sensor\"):\n",
    "                    inp.append(\"fingerprint reader\")\n",
    "                elif(value == 'pen'):\n",
    "                    inp.append(\"stylus\")\n",
    "                else:\n",
    "                    allowed_special.append(value)\n",
    "                    inp.append(value)\n",
    "            else:\n",
    "                inp.append(match_value)\n",
    "\n",
    "        #set value for that record as array of identified features\n",
    "        special_column.iat[i] = inp\n",
    "    \n",
    "    #return shrunk and reformated column\n",
    "    return special_column\n",
    "\n",
    "def special_clean(special_column):\n",
    "    #function to remove features which have cardinality one and are therefore outliers\n",
    "\n",
    "    values = {}\n",
    "\n",
    "    for i in range(len(special_column)):\n",
    "\n",
    "        for feature in special_column.iloc[i]:\n",
    "            #add index of record to associated feature in dictionary\n",
    "            if feature in values:\n",
    "                values[feature].append(i)\n",
    "            else:\n",
    "                values[feature] = [i]\n",
    "    \n",
    "    #remove any features that only appear once from the record that they appear\n",
    "    for x in values.keys():\n",
    "        if len(values[x]) == 1:\n",
    "            special_column.iloc[values[x][0]].remove(x)\n",
    "\n",
    "    #return cleaned column\n",
    "    return special_column\n",
    "\n",
    "def combine_graphics(Features):\n",
    "    #define columns\n",
    "    coprocessor = str(Features['graphics_coprocessor'])\n",
    "    graphics = str(Features['graphics'])\n",
    "\n",
    "    if coprocessor != \"nan\":\n",
    "\n",
    "        #check whether coprocessor column incorrectly used to declare integrated graphics\n",
    "        if (coprocessor == \"intel\") or (coprocessor == \"embedded\"):\n",
    "            return 'integrated'\n",
    "        \n",
    "        #otherwise use dedicated card\n",
    "        return coprocessor\n",
    "    \n",
    "    #return the integrated graphics declared in the 'graphics' column or assume laptop uses integrated graphics\n",
    "    elif (graphics != \"nan\"):\n",
    "        return graphics\n",
    "    else:\n",
    "        return \"integrated\"\n",
    "\n",
    "def graphics_shrink(graphics_column):\n",
    "    #initial set of allowed values\n",
    "    processors = ['integrated']\n",
    "\n",
    "    for i in range(len(graphics_column)):\n",
    "        value = str(graphics_column.iloc[i])\n",
    "        if value == \"nan\":\n",
    "            graphics_column.iat[i] = \"integrated\"\n",
    "            continue\n",
    "\n",
    "        #run fuzzy string matching on value\n",
    "        result = process.extractOne(value, processors)\n",
    "        match_value, score = result\n",
    "\n",
    "        #if matching confidence less than 90% insert value into set of allowed cpus, otherwise use matched value.\n",
    "        if score < 90:\n",
    "            processors.append(value)\n",
    "            graphics_column.iat[i] = value\n",
    "        else:\n",
    "            graphics_column.iat[i] = match_value\n",
    "\n",
    "    #return shrunk column\n",
    "    return graphics_column\n",
    "    \n",
    "def round_to_power_of_2(x, max):\n",
    "    nearest = pow(2, round(math.log(x, 2)))\n",
    "    if nearest > max:\n",
    "        return max\n",
    "    else:\n",
    "        return nearest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete cpu_speed and color column\n",
    "df = df.drop('cpu_speed', axis=1)\n",
    "df = df.drop('color', axis=1)\n",
    "\n",
    "#remove whitespace and decapitalise all strings\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].str.strip()\n",
    "        df[col] = df[col].str.lower()\n",
    "\n",
    "#remove unnecessary keyword and rows without specified model\n",
    "df = df.dropna(subset=['model'])\n",
    "df.drop(df[df['model'].str.lower() == 'nan'].index, inplace=True)\n",
    "\n",
    "#convert all storage to gb, remove strings from screen_size, ram, and harddisk, convert all to floats\n",
    "df['harddisk'] = df['harddisk'].apply(process_storage)\n",
    "for col in ['screen_size', 'ram', 'harddisk', 'price']:\n",
    "    df[col] = df[col].str.replace(r'[^0-9.]', '', regex=True)\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "#fill screensize unknowns with predictions from scikit decision tree regression model and round all screen sizes to 1 decimal place.\n",
    "df.loc[df['screen_size'].isna(), 'screen_size'] = regression_with_price(df, 'screen_size')\n",
    "df['screen_size'] = df['screen_size'].round(1)\n",
    "\n",
    "#fill ram and harddisk values with average and round all ram and harddisk values to nearest power of two in accordance with storage standards\n",
    "ram_mean = df['ram'].mean()\n",
    "df['ram'] = df['ram'].fillna(ram_mean)\n",
    "df['ram'] = df['ram'].apply(lambda x: round_to_power_of_2(x, max=32))\n",
    "\n",
    "harddisk_mean = df['harddisk'].mean()\n",
    "df['harddisk'] = df['harddisk'].fillna(harddisk_mean)\n",
    "df['harddisk'] = df['harddisk'].apply(lambda x: round_to_power_of_2(x, max=2048))\n",
    "\n",
    "\n",
    "#remove model duplicates, then merge the brand and model columns\n",
    "df = df.drop_duplicates(subset=['model'])\n",
    "df['model'] = df.apply(concatenate_brand_model, axis=1)\n",
    "df = df.drop('brand', axis=1)\n",
    "\n",
    "#use custom string matching algorithm to shrink the number of distinct values by standardising similar entries\n",
    "df['cpu'] = cpu_shrink(df['cpu'])\n",
    "\n",
    "#shrink number of possible OS values with fuzzy string matching\n",
    "df['OS'] = os_shrink(df['OS'])\n",
    "\n",
    "#change os to be one of the three main ones by fuzzy string matching\n",
    "df['special_features'] = df['special_features'].str.replace('&',',')\n",
    "df['special_features'] = special_shrink(df['special_features'])\n",
    "df['special_features'] = special_clean(df['special_features'])\n",
    "\n",
    "#combine graphics and graphics_coprocessor columns, where laptops with non integrated graphics have the card specified in the graphics column\n",
    "df['graphics'] = df.apply(combine_graphics, axis=1)\n",
    "\n",
    "#use custom string matching algorithm to shrink the number of distinct values by standardising similar entries\n",
    "df['graphics'] = graphics_shrink(df['graphics'])\n",
    "\n",
    "df = df.drop('graphics_coprocessor', axis=1)\n",
    "\n",
    "# find mean of ratings column and fill all NaN values with this value\n",
    "mean = df['rating'].mean().round(1)\n",
    "df['rating'] = df['rating'].fillna(mean)\n",
    "\n",
    "#remove all records with rating value less than the mean\n",
    "df = df[df['rating'] >= mean]\n",
    "\n",
    "#use decision tree regression to predict values for records with NaN for price\n",
    "df = regression_on_price(df)\n",
    "\n",
    "#drop all records with price greater than 1500 for Q2\n",
    "df = df[df['price'] <= 1500]\n",
    "\n",
    "#drop all records without a cpu listing, this is too important to be omitted and cannot accurately be predicted given current data\n",
    "df = df.dropna(subset=['cpu'])\n",
    "df.drop(df[df['cpu'].str.lower() == 'nan'].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              model  screen_size  harddisk  \\\n",
      "1                             dell inspiron 15 3530         15.6      1024   \n",
      "2                                 dell vostro 3510          15.6      1024   \n",
      "3                         dell inspiron 7420 2-in-1         14.0      2048   \n",
      "4                      msi prestige 14evo a11mo-217         14.0      1024   \n",
      "5                        lenovo thinkbook 15 g4 iap         15.6       512   \n",
      "...                                             ...          ...       ...   \n",
      "4354  acer chromebook enterprise spin 514 cp514-3wh         14.0       256   \n",
      "4441                      dell latitude rugged 5404         14.0       256   \n",
      "4442                                          hps13         15.0       128   \n",
      "4444                             dell latitude 5300         13.3       512   \n",
      "4445                          hp prodesk 400 g3-sff         14.8      1024   \n",
      "\n",
      "          cpu  ram               OS         special_features  \\\n",
      "1     core i7   32  windows 11 home                       []   \n",
      "2     core i7   32  windows 10 home        [wifi, bluetooth]   \n",
      "3     core i5   32   windows 11 pro                       []   \n",
      "4     core i7   32   windows 10 pro      [anti glare, bezel]   \n",
      "5     core i7    8   windows 11 pro             [anti glare]   \n",
      "...       ...  ...              ...                      ...   \n",
      "4354  ryzen 7   16        chrome os  [corning gorilla glass]   \n",
      "4441  core i7    8   windows 10 pro       [backlit keyboard]   \n",
      "4442  pentium    4  windows 11 home               [hd audio]   \n",
      "4444  core i5   16   windows 10 pro               [speakers]   \n",
      "4445  core i5    8   windows 10 pro                       []   \n",
      "\n",
      "                    graphics  rating    price  \n",
      "1     intel iris xe graphics     4.2  1000.00  \n",
      "2                 integrated     4.1  1000.99  \n",
      "3                 integrated     4.1  1001.11  \n",
      "4     intel iris xe graphics     4.2  1001.18  \n",
      "5     intel iris xe graphics     4.1  1002.26  \n",
      "...                      ...     ...      ...  \n",
      "4354     amd radeon graphics     5.0   999.99  \n",
      "4441              integrated     4.3   558.32  \n",
      "4442              integrated     4.3   278.74  \n",
      "4444              integrated     4.2  1139.33  \n",
      "4445              integrated     5.0   899.99  \n",
      "\n",
      "[526 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to New Excel Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(CleanData, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData = \"amazon_laptop_2023_cleaned.xlsx\"\n",
    "# Read the data from the Excel file\n",
    "stand_df = pd.read_excel(CleanData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise dictionary of relative scores of CPUs using effective CPU speed index [of median CPU for a given family if applicable]\n",
    "\n",
    "information from: https://cpu.userbenchmark.com/ , https://versus.com/en , https://www.cpu-monkey.com/en/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPU_scores = {\"core i7\": 0.733, \"core i5\" : 0.648, \"core i3\" : 0.552, \"core i9\" : 0.997, \"amd r series\" : 0.538, \"ryzen 5\" : 0.771, \"ryzen 7\" : 0.881, \"1.2ghz cortex a8 processor\" : 0.301, \"intel mobile cpu\" : 0.478, \"apple m1\" : 0.967, \"snapdragon\" : 0.705, \"mediatek mt8183\" : 0.360, \"celeron\" : 0.212, \"intel atom\" : 0.192, \"pentium\" : 0.453, \"mediatek_mt8127\" : 0.046, \"atom z8350\" : 0.214, \"athlon\" : 0.401, \"core m\" : 0.380, \"ryzen 3\" : 0.74, \"amd a4\" : 0.340, \"mediatek helio p60t\" : 0.594, \"amd kabini a6-5200m quad core\" : 0.304, \"core_m\" : 0.380}\n",
    "\n",
    "# mean = 0\n",
    "# for x in CPU_scores.values():\n",
    "#     mean += x\n",
    "# mean = (mean) / len(CPU_scores.values())\n",
    "\n",
    "# sd = 0\n",
    "# for x in CPU_scores.values():\n",
    "#     sd += (x-mean)**2\n",
    "# sd = math.sqrt(sd / len(CPU_scores.values()))\n",
    "\n",
    "# for x in CPU_scores.keys():\n",
    "#     CPU_scores[x] = (CPU_scores[x] - mean) / sd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise dictionary of relative scores of GPUs using effective GPU speed index\n",
    "\n",
    "information from: https://www.cpu-monkey.com/en/, https://www.notebookcheck.net/, https://cpu.userbenchmark.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_scores = {\"intel iris xe graphics\" : 0.141, \"integrated\" : 0.141, \"amd radeon graphics\" : 0.212, \"nvidia geforce mx250\" : 0.137, \"intel uhd graphics\" : 0.052, \"dedicated\" : 0.141, \"nvidia geforce rtx 3050 ti 4gb gddr6\" : 0.608, \"rtx t600\" : 0.221, \"xps9300-7909slv-pus\" : 0.055, \"amd radeon 680m\" : 0.181, \"nvidia geforce rtx 4050\" : 0.961, \"rtx 3060\" : 0.899, \"intel hd\" : 0.029, \"nvidia quadro t1000\" : 0.241, \"nvidia geforce gtx 1660\" : 0.650, \"iris x graphics\" : 0.141, \"rtx a3000\" : 0.620, \"nvidia rtx a1000\" : 0.300, \"intel xe\" : 0.141, \"amd radeon rtx 3070\" : 1.48, \"intel iris plus\" : 0.092, \"t550\" : 0.169, \"nvidia geforce rtx 2070 super\" : 0.976, \"amd radeon r5\" : 0.021, \"mediatek\" : 0.141, \"powervr gx6250\" : 0.318, \"amd athlon silver\" : 0.141, \"amd radeon vega 3\" : 0.046, \"adreno 618\" : 0.014, \"2gb nvidia geforce mx230 graphics\" : 0.104, \"nvidia geforce rtx 3080 ti\" : 1.33, \"nvidia t600\" : 0.221}\n",
    "\n",
    "# mean = 0\n",
    "# for x in GPU_scores.values():\n",
    "#     mean += x\n",
    "# mean = (mean) / len(GPU_scores.values())\n",
    "\n",
    "# sd = 0\n",
    "# for x in GPU_scores.values():\n",
    "#     sd += (x-mean)**2\n",
    "# sd = math.sqrt(sd / len(GPU_scores.values()))\n",
    "\n",
    "# for x in GPU_scores.keys():\n",
    "#     GPU_scores[x] = (GPU_scores[x] - mean) / sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic Feature Scaling Functions and Mapping Function for CPU/GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def min_max_scale(data):\n",
    "    #scale data to be between 0-1\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def robust_scale(data):\n",
    "    #initialise robust scaler from sklearn library\n",
    "    robust_scaler = RobustScaler()\n",
    "    \n",
    "    #scale data using robust scaler\n",
    "    robust_scaled_data = robust_scaler.fit_transform(np.array(data).reshape(-1, 1)).flatten()\n",
    "\n",
    "    #scale values within interquartile range to between 0-1 (reduces affect of outliers)\n",
    "    iqr_min = np.percentile(robust_scaled_data, 25)\n",
    "    iqr_max = np.percentile(robust_scaled_data, 75)\n",
    "    scaled_iqr = (robust_scaled_data - iqr_min) / (iqr_max - iqr_min)\n",
    "\n",
    "    #return dataset where outliers are rescaled to 0 or 1\n",
    "    return np.where(scaled_iqr > 0.75, 1, np.where(scaled_iqr < 0.25, 0, scaled_iqr))\n",
    "\n",
    "def card_replace(column, dict):\n",
    "    #for each column, map processor to numerical performance\n",
    "    column = column.map(dict)\n",
    "    return column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create New Reconfigured Dataframe so all Columns can be Numerically Considered Based on Z-Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rating column as data is unreliable for scoring\n",
    "stand_df = stand_df.drop([\"rating\"], axis=1)\n",
    "\n",
    "#convert processing card values with associated numerical performance\n",
    "for x in [[\"cpu\", CPU_scores],[\"graphics\", GPU_scores]]:\n",
    "    stand_df[x[0]] = card_replace(stand_df[x[0]], x[1])\n",
    "\n",
    "#min-max scale processor values, useful to allow outliers to skew scale as most laptops will use low performance cards\n",
    "for x in [\"cpu\",\"graphics\"]:\n",
    "        stand_df[f'{x}_score'] = min_max_scale(stand_df[x])\n",
    "\n",
    "#min-max scale price, and invert score (want higher score to indicate lower price)\n",
    "stand_df[\"price_score\"] = 1 - min_max_scale(stand_df[\"price\"])\n",
    "\n",
    "#use custom robust scaling for features which have high variance with low consequence for variance.\n",
    "for x in [\"screen_size\", \"harddisk\", \"ram\"]:\n",
    "        stand_df[f'{x}_score'] = robust_scale(stand_df[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate New Dataframe With Z-Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(df,features={}, os={}, screen_weight=1, harddisk_weight=1, cpu_weight=1, ram_weight=1, graphics_weight=1, price_weight=1):\n",
    "    # create a new DataFrame with scores\n",
    "    scores_df = df.copy()\n",
    "\n",
    "    #calculate scores for each row\n",
    "    scores_df['score'] = (\n",
    "        screen_weight * scores_df['screen_size_score'] +\n",
    "        harddisk_weight * scores_df['harddisk_score'] +\n",
    "        cpu_weight * scores_df['cpu_score'] +\n",
    "        ram_weight * scores_df['ram_score'] +\n",
    "        graphics_weight * scores_df['graphics_score'] +\n",
    "        price_weight * scores_df['price_score']\n",
    "    )\n",
    "    \n",
    "    #increase score for special features\n",
    "    for feature, value in features.items():\n",
    "        scores_df['score'] += value * scores_df['special_features'].apply(lambda x: int(feature in x))\n",
    "\n",
    "    #increase score for os\n",
    "    for os, value in os.items():\n",
    "        scores_df['score'] += value * scores_df['OS'].apply(lambda x: int(os in x))\n",
    "\n",
    "\n",
    "    return scores_df\n",
    "\n",
    "#calculate score and sort dataframe by scores before storing\n",
    "stand_df = calculate_scores(stand_df)\n",
    "stand_df = stand_df.sort_values(by='score', ascending=False)\n",
    "stand_df.to_excel(\"scored-df.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Normalised Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_df = pd.read_excel(\"scored-df.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Modules for Plotting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import CubicSpline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Function to build and Save Plot for Z-Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_save_distribution(column, df):\n",
    "    #generate bins for scores\n",
    "    bins = [float(x) / 20 for x in range(0,20)]\n",
    "\n",
    "    #discretize data into specified bins\n",
    "    df[f'{column}_bucket'] = pd.cut(stand_df[column], bins=bins, labels=['0-0.05','0.05-0.1','0.1-0.15','0.15-0.2','0.2-0.25','0.25-0.3','0.3-0.35','0.35-0.4','0.4-0.45','0.45-0.5','0.5-0.55','0.55-0.6','0.6-0.65','0.65-0.7','0.7-0.75','0.75-0.8','0.8-0.85','0.85-0.9','0.9-0.95','0.95-1',])\n",
    "\n",
    "    #create bar chart using Seaborn\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    chart = sns.countplot(x=f'{column}_bucket', data=df, palette='viridis')\n",
    "\n",
    "    #get bar heights and midpoints\n",
    "    bar_heights = [p.get_height() for p in chart.patches]\n",
    "    bar_midpoints = [(p.get_x() + (p.get_width() / 2)) for p in chart.patches]\n",
    "    \n",
    "    #use cubic spline to create a smooth function passing through the midpoints\n",
    "    cs = CubicSpline(bar_midpoints, bar_heights, bc_type=((2, 0.0), (2, 0.0)))\n",
    "\n",
    "    #generate x values for the smooth curve\n",
    "    x_smooth = np.linspace(min(bar_midpoints), max(bar_midpoints), 1000)\n",
    "\n",
    "    #plot curve\n",
    "    plt.plot(x_smooth, cs(x_smooth), color='orange', label='Distribution Estimate', linewidth=2)\n",
    "\n",
    "    #rotate x-axis labels so visible\n",
    "    chart.set_xticklabels(chart.get_xticklabels(), rotation=75)\n",
    "\n",
    "    #set labels, title, and fit so as to not crop out data\n",
    "    plt.xlabel('Scores')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'{column} Score Distribution')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    #save and show figure\n",
    "    plt.savefig(f'{column}_distribution.pdf')\n",
    "    plt.legend()  # Show legend with smooth curve label\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run for Z-Score Normalised Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lukep\\Documents\\Uni\\Year_2\\Data science\\Data_Science_coursework\\srhp63.ipynb Cell 32\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lukep/Documents/Uni/Year_2/Data%20science/Data_Science_coursework/srhp63.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, col \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mscreen_size\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mharddisk\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mram\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgraphics\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lukep/Documents/Uni/Year_2/Data%20science/Data_Science_coursework/srhp63.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     build_and_save_distribution(col, stand_df)\n",
      "\u001b[1;32mc:\\Users\\lukep\\Documents\\Uni\\Year_2\\Data science\\Data_Science_coursework\\srhp63.ipynb Cell 32\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lukep/Documents/Uni/Year_2/Data%20science/Data_Science_coursework/srhp63.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_and_save_distribution\u001b[39m(column, df):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lukep/Documents/Uni/Year_2/Data%20science/Data_Science_coursework/srhp63.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     bins \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m0.05\u001b[39;49m)]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lukep/Documents/Uni/Year_2/Data%20science/Data_Science_coursework/srhp63.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m#discretize data into specified bins\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lukep/Documents/Uni/Year_2/Data%20science/Data_Science_coursework/srhp63.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     df[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcolumn\u001b[39m}\u001b[39;00m\u001b[39m_bucket\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mcut(stand_df[column], bins\u001b[39m=\u001b[39mbins, labels\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m0-0.05\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.05-0.1\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.1-0.15\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.15-0.2\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.2-0.25\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.25-0.3\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.3-0.35\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.35-0.4\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.4-0.45\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.45-0.5\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.5-0.55\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.55-0.6\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.6-0.65\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.65-0.7\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.7-0.75\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.75-0.8\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.8-0.85\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.85-0.9\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.9-0.95\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m0.95-1\u001b[39m\u001b[39m'\u001b[39m,])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "for x, col in enumerate([\"screen_size\", \"harddisk\", \"cpu\", \"ram\", \"graphics\", \"price\"]):\n",
    "    build_and_save_distribution(col, stand_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Custom Configuration for Overall Scores (not z-score normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.75, 4, 4.25, 4.5, 4.75, 5, 5.25]\n",
    "\n",
    "# Use pd.cut to discretize the data into the specified bins\n",
    "df['score_bucket'] = pd.cut(stand_df['score'], bins=bins, labels = ['0 to 0.25', '0.25 to 0.5', '0.5 to 0.75', '0.75 to 1', '1 to 1.25', '1.25 to 1.5', '1.5 to 1.75', '1.75 to 2', '2 to 2.25', '2.25 to 2.5', '2.5 to 2.75', '2.75 to 3', '3 to 3.25', '3.25 to 3.5', '3.5 to 3.75', '3.75 to 4', '4 to 4.25', '4.25 to 4.5', '4.5 to 4.75', '4.75 to 5', '5 to 5.25'])\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "chart = sns.countplot(x='score_bucket', data=df, palette='viridis')\n",
    "\n",
    "# Get the bar heights and midpoints\n",
    "bar_heights = [p.get_height() for p in chart.patches]\n",
    "bar_midpoints = [(p.get_x() + p.get_width() / 2.) for p in chart.patches]\n",
    "\n",
    "# Use cubic spline to create a smooth function passing through the midpoints\n",
    "cs = CubicSpline(bar_midpoints, bar_heights)\n",
    "\n",
    "# Generate x values for the smooth curve\n",
    "x_smooth = np.linspace(min(bar_midpoints), max(bar_midpoints), 1000)\n",
    "\n",
    "# Plot the smooth curve\n",
    "plt.plot(x_smooth, cs(x_smooth), color='orange', label='Distribution Estimate', linewidth=2)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=75, horizontalalignment='right')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Overall Score Bins')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Overall Score Distribution')\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('score_distribution.pdf')\n",
    "plt.legend()  # Show legend with smooth curve label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Best Laptop for Video-Editor and Travelling Business-man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Editor_df = calculate_scores(df=stand_df, screen_weight=1,harddisk_weight=1.6,cpu_weight=2,ram_weight=2,graphics_weight=2,price_weight=0.5)\n",
    "# Editor_df_df = Editor_df.sort_values(by='score', ascending=False)\n",
    "# #print(Editor_df)\n",
    "# Business_df = calculate_scores(df=stand_df, features={\"backlit keyboard\" : 0.3, \"lightweight\" : 0.3, \"corning gorilla glass\" : 0.2, \"spill resistant\" : 0.2}, os={\"windows 11 pro\": 0.4, \"windows 10 pro\" : 0.35, \"mac os\" : 0.3}, screen_weight=-0.3,harddisk_weight=1,cpu_weight=1,ram_weight=1,graphics_weight=1,price_weight=1.5)\n",
    "# Business_df = Business_df.sort_values(by='score', ascending=False)\n",
    "# print(Business_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
